{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fdb2bb",
   "metadata": {},
   "source": [
    "# üåç GeoViT: A Convolutional-Transformer Model for Geolocation Estimation\n",
    "\n",
    "Welcome to the GeoViT project notebook!\n",
    "\n",
    "This notebook presents the training, evaluation, and experimentation pipeline for **GeoViT**, a neural network model designed to **predict geographic locations from Google Street View images**. The model takes inspiration from the popular game *Geoguessr* and is trained using the [OpenStreetView-5M dataset](https://huggingface.co/datasets/osv5m/osv5m).\n",
    "\n",
    "üñäÔ∏è Authors: Alan Tran and Caleb Wolf\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Project Goals\n",
    "\n",
    "1. **Train** a hybrid convolutional-transformer model that can learn geospatial patterns from street-level imagery.\n",
    "2. **Evaluate** the model using geodesic distance-based metrics.\n",
    "3. **Experiment** with:\n",
    "   - Vision Transformer ablations (layers & attention heads)\n",
    "   - Robustness to reduced image context (square vs 3:2 aspect ratio)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Model Overview\n",
    "\n",
    "- **Convolutional Frontend:** Captures local texture and object-level features.\n",
    "- **Vision Transformer (ViT):** Captures global spatial dependencies.\n",
    "- **Output:** Regressed GPS coordinates (Latitude, Longitude)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Experiments\n",
    "\n",
    "### ‚úÖ Experiment 1: ViT Ablation\n",
    "- Reduce number of transformer layers and attention heads\n",
    "- Assess contribution of transformer structure to geolocation performance\n",
    "\n",
    "### ‚úÖ Experiment 2: Robustness to Cropped Context\n",
    "- Evaluate model on square images (less context)\n",
    "- Compare against standard aspect ratio input\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alan\\anaconda3\\envs\\NN1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from s2sphere import LatLng, CellId\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "DATA_ROOT = './osv5m/'\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, 'train_mini.csv')\n",
    "TEST_CSV = os.path.join(DATA_ROOT, 'test_mini.csv')\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_ROOT, 'train_images')\n",
    "TEST_IMG_DIR = os.path.join(DATA_ROOT, 'test_images')\n",
    "\n",
    "# Set global parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "L = 10\n",
    "LABEL_COL = 's2_cell'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# New column maps images to an s2 grid cell\n",
    "def add_s2_cell_column(\n",
    "    df: pd.DataFrame,\n",
    "    lat_col: str = 'latitude',\n",
    "    lon_col: str = 'longitude',\n",
    "    level: int = 10,\n",
    "    new_col: str = 's2_cell'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of `df` with a new column `new_col` containing the S2 cell\n",
    "    token (hex) at the specified `level` for each (lat_col, lon_col) pair.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # compute tokens with a list comprehension for speed\n",
    "    tokens = [\n",
    "        CellId.from_lat_lng(LatLng.from_degrees(lat, lon))\n",
    "              .parent(level)\n",
    "              .to_token()\n",
    "        for lat, lon in zip(df[lat_col], df[lon_col])\n",
    "    ]\n",
    "    df[new_col] = tokens\n",
    "    return df\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df = add_s2_cell_column(pd.read_csv(TRAIN_CSV), level=L, new_col=LABEL_COL)\n",
    "test_df = add_s2_cell_column(pd.read_csv(TEST_CSV), level=L, new_col=LABEL_COL)\n",
    "\n",
    "# Build a global mapping from cell ‚Üí index using the training CSV\n",
    "cells, classes = pd.factorize(train_df[LABEL_COL])\n",
    "class_to_idx = {cell: idx for idx, cell in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23799d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN + ViT hybrid model for geospatial classification\n",
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Conv feature extractor (ResNet50)\n",
    "        self.cnn = timm.create_model(\"resnet50\", pretrained=True, features_only=True)\n",
    "        cnn_out_channels = self.cnn.feature_info[-1]['num_chs']\n",
    "\n",
    "        # ViT block (tiny patch-based attention)\n",
    "        self.vit = timm.create_model(\"vit_small_patch16_224\", pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # remove classifier\n",
    "\n",
    "        # Fusion + Classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "        self.proj = nn.Linear(cnn_out_channels, self.vit.embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2) # Dropout Regularization\n",
    "        self.classifier = nn.Linear(self.vit.embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get last feature map from CNN\n",
    "        x = self.cnn(x)[-1]  # shape (B, C, H, W)\n",
    "\n",
    "        # Pool to fixed 14 x 14 size\n",
    "        x = self.pool(x)  # shape (B, C, 14, 14)\n",
    "\n",
    "        # Flatten and transpose to patch seq format that matches ViT input\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, C, H*W) -> (B, H*W, C)\n",
    "\n",
    "        # Project to ViT embedding dim\n",
    "        x = self.proj(x)  # shape (B, 196, D)\n",
    "\n",
    "        # Feed through ViT encoder blocks\n",
    "        x = self.vit.blocks(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        return self.classifier(x)\n",
    "    \n",
    "# Define the geospatial dataset class\n",
    "class GeoDataset(Dataset):\n",
    "    def __init__(self, csv_path, images_root, class_to_idx, transforms=None):\n",
    "        # load annotations\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "        # map 'cell' to the consistent label index; drop any rows not seen in training\n",
    "        self.df['label'] = self.df['cell'].map(class_to_idx)\n",
    "        self.df = self.df[self.df['label'].notna()].reset_index(drop=True)\n",
    "        self.df['label'] = self.df['label'].astype(int)\n",
    "\n",
    "        # share the same classes list\n",
    "        self.classes = classes\n",
    "\n",
    "        # build a map from image‚ÄêID ‚Üí full path\n",
    "        all_files = glob.glob(os.path.join(images_root, '*', '*.jpg'))\n",
    "        self.id2path = {\n",
    "            os.path.splitext(os.path.basename(p))[0]: p\n",
    "            for p in all_files\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row    = self.df.iloc[idx]\n",
    "        img_id = str(row['id'])\n",
    "        label  = int(row['label'])\n",
    "        img    = Image.open(self.id2path[img_id]).convert('RGB')\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24172788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for training data augmentation (better generalization)\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomApply([\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02)\n",
    "    ], p=0.5)  # apply 50% of the time variations in color (simulates lighting changes)\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# Transformations for test data (no augmentation)\n",
    "test_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate the dataset and dataloaders\n",
    "train_ds = GeoDataset(TRAIN_CSV, TRAIN_IMG_DIR, class_to_idx, train_transforms)\n",
    "test_ds  = GeoDataset(TEST_CSV,  TEST_IMG_DIR,  class_to_idx, test_transforms)\n",
    "\n",
    "num_val = int(0.1 * len(train_ds)) # 90% training set, 10% testing set\n",
    "num_train = len(train_ds) - num_val\n",
    "train_subset, val_subset = random_split(train_ds, [num_train, num_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,      batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Model\n",
    "model = CNN_ViT_Hybrid(num_classes=len(train_ds.classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Freeze ResNet\n",
    "for param in model.cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze ViT\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create optimizer only for trainable layers (ViT projection + classifier)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4, \n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions of training batch\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "# --- Training Loop ---\n",
    "for epoch in range(4, EPOCHS):\n",
    "    print(f\"\\nüåç Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # Unfreeze layers after 3 epochs\n",
    "    if epoch == 3:\n",
    "        print(\"üîì Unfreezing layers...\")\n",
    "        for param in model.cnn.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.vit.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5\n",
    "        )\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc     = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss or val_acc > best_val_acc:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), f\"hybrid_best_model_epoch{epoch+1}.pth\")\n",
    "        print(\"‚úÖ Saved best model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437872f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Test ---\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"\\n‚úÖ Final Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
