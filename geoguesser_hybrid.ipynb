{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fdb2bb",
   "metadata": {},
   "source": [
    "# üåç GeoViT: A Convolutional-Transformer Model for Geolocation Estimation\n",
    "\n",
    "Welcome to the GeoViT project notebook!\n",
    "\n",
    "This notebook presents the training, evaluation, and experimentation pipeline for **GeoViT**, a neural network model designed to **predict geographic locations from Google Street View images**. The model takes inspiration from the popular game *Geoguessr* and is trained using the [OpenStreetView-5M dataset](https://huggingface.co/datasets/osv5m/osv5m).\n",
    "\n",
    "üñäÔ∏è Authors: Alan Tran and Caleb Wolf\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Project Goals\n",
    "\n",
    "1. **Train** a hybrid convolutional-transformer model that can learn geospatial patterns from street-level imagery.\n",
    "2. **Evaluate** the model using geodesic distance-based metrics.\n",
    "3. **Experiment** with:\n",
    "   - Vision Transformer ablations (layers & attention heads)\n",
    "   - Robustness to reduced image context (square vs 3:2 aspect ratio)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Model Overview\n",
    "\n",
    "- **Convolutional Frontend:** Captures local texture and object-level features.\n",
    "- **Vision Transformer (ViT):** Captures global spatial dependencies.\n",
    "- **Output:** Regressed GPS coordinates (Latitude, Longitude)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Experiments\n",
    "\n",
    "### ‚úÖ Experiment 1: ViT Ablation\n",
    "- Reduce number of transformer layers and attention heads\n",
    "- Assess contribution of transformer structure to geolocation performance\n",
    "\n",
    "### ‚úÖ Experiment 2: Robustness to Cropped Context\n",
    "- Evaluate model on square images (less context)\n",
    "- Compare against standard aspect ratio input\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "DATA_ROOT = './osv5m/'\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, 'train_mini.csv')\n",
    "TEST_CSV = os.path.join(DATA_ROOT, 'test_mini.csv')\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_ROOT, 'train_images')\n",
    "TEST_IMG_DIR = os.path.join(DATA_ROOT, 'test_images')\n",
    "\n",
    "# Set hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) Build a global mapping from cell ‚Üí index using the training CSV\n",
    "train_df    = pd.read_csv(TRAIN_CSV)\n",
    "cells, classes  = pd.factorize(train_df['cell'])\n",
    "class_to_idx    = {cell: idx for idx, cell in enumerate(classes)}\n",
    "\n",
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Conv feature extractor (ConvNeXt-T or ResNet50)\n",
    "        self.cnn = timm.create_model(\"resnet50\", pretrained=True, features_only=True)\n",
    "        cnn_out_channels = self.cnn.feature_info[-1]['num_chs']\n",
    "\n",
    "        # ViT block (tiny patch-based attention)\n",
    "        self.vit = timm.create_model(\"vit_small_patch16_224\", pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # remove classifier\n",
    "\n",
    "        # Fusion + Classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "        self.proj = nn.Linear(cnn_out_channels, self.vit.embed_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(self.vit.embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN backbone\n",
    "        x = self.cnn(x)[-1]  # shape (B, C, H, W)\n",
    "\n",
    "        # Pool to fixed size\n",
    "        x = self.pool(x)  # shape (B, C, 14, 14)\n",
    "\n",
    "        # Flatten and transpose to match ViT input\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, C, H*W) -> (B, H*W, C)\n",
    "\n",
    "        # Project to ViT embedding dim\n",
    "        x = self.proj(x)  # shape (B, 196, D)\n",
    "\n",
    "        # Feed through ViT encoder blocks\n",
    "        x = self.vit.blocks(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23799d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Dataset =====\n",
    "class GeoDataset(Dataset):\n",
    "    def __init__(self, csv_path, images_root, class_to_idx, transforms=None):\n",
    "        # load annotations\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "        # map 'cell' to the consistent label index; drop any rows not seen in training\n",
    "        self.df['label'] = self.df['cell'].map(class_to_idx)\n",
    "        self.df = self.df[self.df['label'].notna()].reset_index(drop=True)\n",
    "        self.df['label'] = self.df['label'].astype(int)\n",
    "\n",
    "        # share the same classes list\n",
    "        self.classes = classes\n",
    "\n",
    "        # build a map from image‚ÄêID ‚Üí full path\n",
    "        all_files = glob.glob(os.path.join(images_root, '*', '*.jpg'))\n",
    "        self.id2path = {\n",
    "            os.path.splitext(os.path.basename(p))[0]: p\n",
    "            for p in all_files\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row    = self.df.iloc[idx]\n",
    "        img_id = str(row['id'])\n",
    "        label  = int(row['label'])\n",
    "        img    = Image.open(self.id2path[img_id]).convert('RGB')\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24172788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transforms ---\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Load Data ---\n",
    "train_ds = GeoDataset(TRAIN_CSV, TRAIN_IMG_DIR, class_to_idx, train_transforms)\n",
    "test_ds  = GeoDataset(TEST_CSV,  TEST_IMG_DIR,  class_to_idx, test_transforms)\n",
    "\n",
    "num_val = int(0.2 * len(train_ds))\n",
    "num_train = len(train_ds) - num_val\n",
    "train_subset, val_subset = random_split(train_ds, [num_train, num_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,      batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Model ---\n",
    "model = CNN_ViT_Hybrid(num_classes=len(train_ds.classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer only for trainable layers (ViT + classifier)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training & Eval ---\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "model.load_state_dict(torch.load(\"no-freeze\\hybrid_best_model_epoch4.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "# --- Training Loop ---\n",
    "for epoch in range(4, EPOCHS):\n",
    "    print(f\"\\nüåç Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc     = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss or val_acc > best_val_acc:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), f\"hybrid_best_model_epoch{epoch+1}.pth\")\n",
    "        print(\"‚úÖ Saved best model.\")\n",
    "\n",
    "# --- Final Test ---\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"\\n‚úÖ Final Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune with frozen layers\n",
    "# Load the best model from the previous training\n",
    "model.load_state_dict(torch.load('no-freeze\\hybrid_best_model_epoch5.pth'))\n",
    "model.to(device)\n",
    "\n",
    "for param in model.cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Only params that require gradients\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-5) # reinitialize optimizer\n",
    "\n",
    "# --- Fine-tuning with frozen layers ---\n",
    "fine_tune_epochs = 3\n",
    "start_epoch = 6\n",
    "best_val_loss = 0.4936\n",
    "best_val_acc = 0.8584\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + fine_tune_epochs):\n",
    "    print(f\"\\nüéØ Fine-tuning Epoch {epoch}/{start_epoch + fine_tune_epochs - 1}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc     = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss or val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), f\"hybrid_best_model_epoch{epoch+1}.pth\")\n",
    "        print(\"‚úÖ Saved best model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
