{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2005a3",
   "metadata": {},
   "source": [
    "# üåç GeoViT: A Convolutional-Transformer Model for Geolocation Estimation\n",
    "\n",
    "Welcome to the GeoViT project notebook!\n",
    "\n",
    "This notebook presents the training, evaluation, and experimentation pipeline for **GeoViT**, a neural network model designed to **predict geographic locations from Google Street View images**. The model takes inspiration from the popular game *Geoguessr* and is trained using the [OpenStreetView-5M dataset](https://huggingface.co/datasets/osv5m/osv5m).\n",
    "\n",
    "üñäÔ∏è Authors: Alan Tran and Caleb Wolf\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Project Goals\n",
    "\n",
    "1. **Train** a hybrid convolutional-transformer model that can learn geospatial patterns from street-level imagery.\n",
    "2. **Evaluate** the model using geodesic distance-based metrics.\n",
    "3. **Experiment** with:\n",
    "   - Vision Transformer ablations (layers & attention heads)\n",
    "   - Robustness to reduced image context (square vs 3:2 aspect ratio)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Model Overview\n",
    "\n",
    "- **Convolutional Frontend:** Captures local texture and object-level features.\n",
    "- **Vision Transformer (ViT):** Captures global spatial dependencies.\n",
    "- **Output:** Regressed GPS coordinates (Latitude, Longitude)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Experiments\n",
    "\n",
    "### ‚úÖ Experiment 1: ViT Ablation\n",
    "- Reduce number of transformer layers and attention heads\n",
    "- Assess contribution of transformer structure to geolocation performance\n",
    "\n",
    "### ‚úÖ Experiment 2: Robustness to Cropped Context\n",
    "- Evaluate model on square images (less context)\n",
    "- Compare against standard aspect ratio input\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d34ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"osv5m/osv5m\", local_dir=\"datasets/osv5m\", repo_type='dataset')\n",
    "import os\n",
    "import zipfile\n",
    "for root, dirs, files in os.walk(\"datasets/osv5m\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(os.path.join(root, file), 'r') as zip_ref:\n",
    "                zip_ref.extractall(root)\n",
    "                os.remove(os.path.join(root, file))\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('osv5m/osv5m', full=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
